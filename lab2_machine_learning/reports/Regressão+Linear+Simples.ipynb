{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Linear Multipla com NumPy e Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo ferei um paralelo entre três implementações de regressão multipla em Python. Uma utilizará o algoritmo iterativo, outra fará uso da fórmula fechada e as duas ultimas irão usar bibliotecas específicas chamadas Sklearn e NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados\n",
    "\n",
    "Foi utilizado um conjunto de dados com seis colunas numéricas. As cinco primeiras descrevem as notas em avaliações do primeiro semestre para um conjunto de alunos. A ultima coluna de dados representa o CRA do mesmo dado aluno ao concluir o curso em questão.\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "O objetivo aqui é construir um modelo de descrição utilizando Regressão Linear Multipla para os dados apresentados e comparar o desempenho das quatro alternativas de implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import numpy.linalg as lin\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Norma L2\n",
    "\n",
    "$||\\mathbf{w}||_2 = \\sqrt{\\sum_{j=1}^D w_j^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_2(x):\n",
    "    c=0\n",
    "    for i in range(len(x)):\n",
    "        c += x[i]**2\n",
    "    return math.sqrt(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE (Versão Vetorizada)\n",
    "\n",
    "Função para calcular o MSE (Mean Squared Error):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MSE(\\hat{w})=\\frac{1}{N}(y-\\mathbf{H}\\hat{\\mathbf{w}})^T(y-\\mathbf{H}\\hat{\\mathbf{w}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_vectorized(w,H,Y):\n",
    "    res = Y - np.dot(H,w)\n",
    "    totalError = np.dot(res.T,res)\n",
    "    return totalError / float(len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão multipla\n",
    "\n",
    "\n",
    "### Algoritmo de Gradiente Descendente (versão vetorizada)\n",
    "\n",
    "#### Atualização dos coeficientes\n",
    "\n",
    "Função para fazer uma atualização dos parâmetros no Gradiente Descendente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w^{(t+1)} = {\\displaystyle \\nabla }w^{(t)} + 2\\alpha[H^T(y - Hw)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient_vectorized(w_current,H,Y,learningRate):\n",
    "    res = Y - np.dot(H,w_current)\n",
    "    gradient = -2 * np.dot((H.transpose()),res)\n",
    "    new_w = w_current - learningRate * gradient\n",
    "    return [gradient,new_w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laço de Iteração)\n",
    "\n",
    "Função para iterar sobre o gradiente descendente até convergência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_runner_vectorized(starting_w, H,Y, learning_rate, epsilon):\n",
    "    w = starting_w\n",
    "    gradient = np.array([[np.inf]])\n",
    "    for i in range(0, len(H[0,:])-2):\n",
    "        gradient = gradient[:][:,np.newaxis]\n",
    "    i = 0\n",
    "    while (np.linalg.norm(gradient)>=epsilon):\n",
    "        gradient,w = step_gradient_vectorized(w, H, Y, learning_rate)\n",
    "        i+= 1\n",
    "    return [gradient,w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente descendente (fórmula fechada)\n",
    "\n",
    "Função que retorna o Gradiente Descendente de forma fechada:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat w = (H^TH)^{-1}H^Ty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_runner_vectorized_fixed(starting_w, H,Y, learning_rate):\n",
    "    w = np.dot(lin.inv(np.dot(H.transpose(),H)),np.dot(H.transpose(),Y))\n",
    "    gradient,w_aux = step_gradient_vectorized(w, H, Y, learning_rate)\n",
    "    return [gradient,w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método Principal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Começando com MSE = [[54.47995386]]\n",
      "Vamos lá...\n",
      "\n",
      "\n",
      "*********SKLEARN*********\n",
      "\n",
      "\n",
      "\n",
      "Coeficientes:  [[1.73771151 0.10304143 0.0464367  0.16409834 0.38117843 0.02027816]]\n",
      "Norma dos coeficientes:  1.7902658105305211\n",
      "MSE:  [[0.41133759]]\n",
      "Tempo de processamento: 0.9999275207519531 ms\n",
      "\n",
      "\n",
      "*********NUMPY*********\n",
      "\n",
      "\n",
      "\n",
      "Coeficientes:  [1.7377115137944403, 0.10304143246259861, 0.04643670085073445, 0.16409834419165809, 0.3811784266558142, 0.020278157624844473]\n",
      "Norma dos coeficientes:  1.790265810530524\n",
      "MSE:  [[0.41133759]]\n",
      "Tempo de processamento: 59.999704360961914 ms\n",
      "\n",
      "\n",
      "*********FÓRMULA FECHADA*********\n",
      "\n",
      "\n",
      "\n",
      "Coeficientes:  [1.7377115137945367, 0.10304143246259034, 0.04643670085073254, 0.16409834419164415, 0.3811784266558056, 0.020278157624833426]\n",
      "Norma dos coeficientes:  1.7902658105306137\n",
      "MSE:  [[0.41133759]]\n",
      "Tempo de processamento: 1.0020732879638672 ms\n",
      "\n",
      "\n",
      "*********ALGORITMO ITERATIVO*********\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tclem\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes:  [1.3307123115082506, 0.11033427195945412, 0.06506237372409354, 0.16317101458364705, 0.4018454461283172, 0.025286660062168665]\n",
      "Norma dos coeficientes:  1.4056834557589737\n",
      "MSE:  [[0.41249678]]\n",
      "Tempo de processamento: 3540.961742401123 ms\n"
     ]
    }
   ],
   "source": [
    "points = np.genfromtxt(\"../data/sample_treino.csv\", delimiter=\",\")\n",
    "points = np.c_[np.ones(len(points)),points]\n",
    "H = points[:,0:-1]\n",
    "Y = points[:,-1][:,np.newaxis]\n",
    "init_w = np.zeros((len(points[0,:])-1,1))\n",
    "learning_rate = 0.00002\n",
    "epsilon = 0.5\n",
    "\n",
    "print(\"Começando com MSE = {0}\".format(compute_mse_vectorized(init_w, H,Y)))\n",
    "print(\"Vamos lá...\")\n",
    "print('\\n')\n",
    "\n",
    "# *** sklearn ***\n",
    "print('*********SKLEARN*********')\n",
    "print('\\n\\n')\n",
    "regr = linear_model.LinearRegression()\n",
    "tic = time.time()\n",
    "regr.fit(H, Y)\n",
    "toc = time.time()\n",
    "# incuindo o \"intercet\" no array com os demais coeficientes\n",
    "w_ = regr.coef_\n",
    "w_[0,0] = regr.intercept_\n",
    "\n",
    "print('Coeficientes: ', w_)\n",
    "print('Norma dos coeficientes: ', np.linalg.norm(w_))\n",
    "print('MSE: ', compute_mse_vectorized(w_.transpose(),H,Y))\n",
    "print(\"Tempo de processamento: \" + str(1000*(toc-tic)) + \" ms\")\n",
    "print('\\n')\n",
    "\n",
    "# *** numpy ***\n",
    "print('*********NUMPY*********')\n",
    "print('\\n\\n')\n",
    "tic = time.time()\n",
    "w0 = lin.lstsq(H,Y)[0]\n",
    "toc = time.time()\n",
    "#redistribuindo array de duas dimensões em uma só\n",
    "print('Coeficientes: ', [w0[0,0],w0[1,0],w0[2,0],w0[3,0],w0[4,0],w0[5,0]])\n",
    "print('Norma dos coeficientes: ', np.linalg.norm([w0[0,0],w0[1,0],w0[2,0],w0[3,0],w0[4,0],w0[5,0]]))\n",
    "print('MSE: ', compute_mse_vectorized(w0,H,Y))\n",
    "print(\"Tempo de processamento: \" + str(1000*(toc-tic)) + \" ms\")\n",
    "print('\\n')\n",
    "\n",
    "# *** fórmula fechada ***\n",
    "print('*********FÓRMULA FECHADA*********')\n",
    "print('\\n\\n')\n",
    "tic = time.time()\n",
    "gradient,w2 = gradient_descent_runner_vectorized_fixed(init_w, H,Y, learning_rate)\n",
    "toc = time.time()\n",
    "print('Coeficientes: ', [w2[0,0],w2[1,0],w2[2,0],w2[3,0],w2[4,0],w2[5,0]] )\n",
    "print('Norma dos coeficientes: ', np.linalg.norm(w2))\n",
    "print('MSE: ', compute_mse_vectorized(w2,H,Y))\n",
    "print(\"Tempo de processamento: \" + str(1000*(toc-tic)) + \" ms\")\n",
    "print('\\n')\n",
    "#print(\"Gradiente descendente convergiu com gradient norm = {0}, error = {1}\".format(np.linalg.norm(w2), compute_mse_vectorized(w2,H,Y)))\n",
    "#print(\"Versão vetorizada rodou em: \" + str(1000*(toc-tic)) + \" ms\")\n",
    "\n",
    "\n",
    "# *** algoritmo ***\n",
    "print('*********ALGORITMO ITERATIVO*********')\n",
    "print('\\n\\n')\n",
    "tic = time.time()\n",
    "gradient,w = gradient_descent_runner_vectorized(init_w, H,Y, learning_rate, epsilon)\n",
    "toc = time.time()\n",
    "print('Coeficientes: ', [w[0,0],w[1,0],w[2,0],w[3,0],w[4,0],w[5,0]])\n",
    "print('Norma dos coeficientes: ', np.linalg.norm(w))\n",
    "print('MSE: ', compute_mse_vectorized(w,H,Y))\n",
    "print(\"Tempo de processamento: \" + str(1000*(toc-tic)) + \" ms\")\n",
    "\n",
    "\n",
    "#print(\"Gradiente descendente convergiu com gradient norm = {0}, error = {1}\".format(np.linalg.norm(w), compute_mse_vectorized(w,H,Y)))\n",
    "#print(\"Versão vetorizada rodou em: \" + str(1000*(toc-tic)) + \" ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sumário\n",
    "\n",
    "1. Numa primeira observação já podemos perceber que, à exceção do Algoritmo Iterativo, todos os métodos utilizados chegaram a praticamente os mesmos valores, em tempos variados.\n",
    "2. Os três primeiros métodos variam bastantem em tempo de execução, mas o método mais eficiente tende a ser a fórmula fechada.\n",
    "3. O Algoritmo Iterativo é muito menos eficiente que os demais. Ao passo que todos os demais operam abaixo dos 20 ms, este precisa de aproximadamente 3.5s (na máquina onde este notebook foi testado) para executar. Além disto, ele é o único que diverge significativamente quanto aos resultados.\n",
    "\n",
    "## Resultados\n",
    "\n",
    "Tomando o MSE obtido pela fórmula fechada (muito semelhante a Numpy e Sklearn), podemos dizer que a Regressão multipla descreve os dados de treino à uma taxa de erro de 0.41 aproximadamente. Isto significa usandoo modelo obtido e as notas do primeiro semestre de um dado aluno, podemos apontar seu CRA ao concluir o curso à um erro médio de $|0.41|$ aproximadamente.\n",
    "\n",
    "É importante mencionar que este modelo represeta a os dados de treinamento. Não foram reservados dados para validação ou testes, portanto não se trata e um modelo de predição, apenas descrição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
